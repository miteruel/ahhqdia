{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca9056-9d4d-4028-979e-2bcf5cb4c37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f14b8ef-1ffb-420a-82d7-73d69d0dd7b3",
   "metadata": {},
   "source": [
    "# MoCoPo \n",
    "\n",
    "El protocolo MCP fue estandarizado por Anthropic en noviembre de 2024. \n",
    "https://modelcontextprotocol.io/docs/getting-started/intro\n",
    "\n",
    "Aunque es muy joven (aún no tiene un año cuando escribo esto) cada vez más modelos LLM y programas lo usan (aparte de Claude). \n",
    "\n",
    "https://developer.chrome.com/blog/chrome-devtools-mcp?hl=es-419\n",
    "\n",
    "\n",
    "MCP tiene un futuro prometedor. \n",
    "Pero ¿ para que sirve? , ¿ como y porque funciona?\n",
    "\n",
    "El texto es para personas que conocen poco o nada de MCP , Agentes y herramientas (que son cosas relacionadas).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200d89c-a32a-4d21-b39d-44f2b5da3f6a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## LLM\n",
    "\n",
    "    De forma muy abstracta, cualquier modelo de inteligencia artificial es una gran matriz “numérica”, que hace operaciones con otra matriz numérica(entrada), dando como resultado una nueva matriz de números: .\n",
    "\n",
    "\\* \\* En los modelos de lenguaje (LLM), la entrada es un texto que se codifica con embeddings en matrices numericas , y el resultado se re-codifica a texto. Y lo mismo ocurre si hablamos de imagen o sonido. \n",
    "\n",
    "![image.png](llmabstract.png)\n",
    "\n",
    "\n",
    "Son una especie de caja “cerrada”. No tiene acceso al exterior. Su conocimiento está “encerrado” en la matriz de números de esa caja.  La única forma de comunicarse con esa caja es la entrada, y la caja produce un resultado como  salida. \n",
    "\n",
    "La salida puede cambiar porque en la inferencia de redes neuronales existe una aleatoriedad intrínseca, y además se puede dar más o menos “calor” para que las respuestas sean o más “creativas” o más “predecibles\".\n",
    "\n",
    "## Agentes \n",
    "\n",
    "La forma típica de comunicarse con el LLM, es un programa estilo “chat” en el que introducimos preguntas y se visualizan las respuestas del LLM.  Este programa sabe como comunicarse con el modelo LLM , como enviarle los datos y cómo mostrar los resultados.\n",
    "\n",
    "* Este programa hace de mediador entre el modelo LLM y el usuario humano.  Le podriamos llamar \"agente chat\"\n",
    "* De la misma forma podemos crear programas mediadores entre el LLM y nuestros programas, que serian otros tipos de Agente, sin intervención humana.\n",
    "* Sin entrar en dogmatismos de nombres : Lo importante a entender es que una cosa es el LLM (por ejemplo ollama) que llamaremos servidor LLM . Y otra cosa son los programas que lo usan y que pueden combinarse. Desde el propio programa chat, a los programas añadidos a los que  podriamos llamar \"Agentes\".\n",
    "\n",
    "\n",
    "## Herramientas\n",
    "\n",
    "La mayoría de modelos LLM “actuales”, son muy modulares. Algunas características o funciones se activan  según el trabajo a realizar. \n",
    "\n",
    "Por ejemplo , un LLM multimodal que reconoce imágenes no ejecutará la función de OCR de una imagen, si no hay imagen a  analizar. En muchos casos el  agente chat le da ordenes concretas al llm para activar algunas caracteristicas. En otros casos puede ser que el propio LLM active esas  “herramientas internas”, porque sabe cuándo o cómo usarlas.\n",
    "\n",
    "Lo que buscamos, es añadir nuevas herramientas al LLM. O dicho de otra manera, que el LLM pueda hacer cosas que no tenga programadas internamente.\n",
    "\n",
    "Y como el LLM no se puede cambiar, las caracteristicas se añaden a traves de los agentes que lo usan.\n",
    "\n",
    "Una explicación mas detallada y \"normalizada\" de agentes y herramientas\n",
    "\n",
    "https://huggingface.co/learn/agents-course/es/unit0/introduction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d21f212-da16-4fb9-bac7-38ae1a066940",
   "metadata": {},
   "source": [
    "### ollama-python\n",
    "https://github.com/ollama/ollama-python\n",
    "\n",
    "\n",
    "En esta primera parte me centraré en ver como funcionan las herramientas tools en ollama. Asumiendo que funcionan de forma parecida en otros modelos, pero cada uno con sus especificaciones.  \n",
    "\n",
    "La existencia de las tools es anterior al estandar MCP. Pero si entendemos como funcionan las tools, entenderemos MCP.\n",
    "\n",
    "Usaremos la libreria oficial de ollama-python para dar los primeros pasos.\n",
    "Si necesitas instalarla : pip install  ollama  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45e3a2e-71d1-4cda-b31a-ad1fbd065f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install typing-extensions ? si no lo tienes instalado\n",
    "# !pip install --force-reinstall ollama==0.4. ? si queremos una version concreta\n",
    "# !pip install --force-reinstall   ollama "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f488ef-6168-4255-a537-02a212f055d7",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2712acb5-abbc-451e-9b20-77ab26097702",
   "metadata": {},
   "source": [
    "Lo normal es que tengais ollama instalado localmente. \n",
    "\n",
    "Pero ahora tienes la posibilidad de ejecutarlo en la nuve.\n",
    "Puedes tener tu ApiKey accediendo a tu cuenta en \n",
    "https://ollama.com\n",
    "\n",
    "![image.png](ollamaconfig.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d872b8-acec-47cc-939c-e7057574b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la funciones que crean los clientes ollama que se ajusten a las necesidades.\n",
    "\n",
    "from ollama import Client, chat\n",
    "\n",
    "modelo='llama3.2'  \n",
    "\n",
    "# para acceder a tu ollama local\n",
    "def  clientLocal ():\n",
    "    return  Client(host='http://localhost:11434')\n",
    "\n",
    "# para acceder a un ollama instalado en otra maquina (la mia)\n",
    "def  clientRemoto ():\n",
    "    return  Client(host='http://88.6.74.122:11433')\n",
    "\n",
    "# para acceder a un ollama en la nuve\n",
    "def  clientCloud ():\n",
    "    global modelo\n",
    "    modelo=\"gpt-oss:20b-cloud\"\n",
    "    ApiKey=\"xxxx\"\n",
    "    return  Client(\n",
    "              host='https://ollama.com/',\n",
    "              headers={ 'Authorization': 'Bearer ' + ApiKey}    \n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d851a-2b6e-4011-b34c-49c916c1764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = clientLocal()\n",
    "\n",
    "# la version Cloud - \n",
    "# client = clientCloud()\n",
    "# escogemos la version remota\n",
    "client = clientRemoto()\n",
    "\n",
    "\n",
    "\n",
    "def log (*args):\n",
    "   print(*args)\n",
    "   return\n",
    "\n",
    "\n",
    "# este es la primera version de chat \n",
    "def runPrompt (prom):\n",
    "    log('Pregunta:',prom)\n",
    "    response =  client.chat(\n",
    "        model=modelo,\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': prom}]\n",
    "    )    \n",
    "    # Extrae la respuesta \n",
    "    respuesta = response.message.content\n",
    "    log('Respuesta:',respuesta)\n",
    "    return respuesta\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b1c84a-460c-4113-8067-91ff60675ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos que la conexion funciona, \n",
    "\n",
    "def ejemplo1 ():\n",
    "    runPrompt( 'Hola. ¿Donde esta teruel?')\n",
    "\n",
    "ejemplo1()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e96f7-5b0c-4603-a84d-2845914b6e9e",
   "metadata": {},
   "source": [
    "\n",
    "Ahora vamos a jugar un poco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350b9cd-cc5f-46be-9302-ea96564c1475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ejemplo2 ():\n",
    "    runPrompt( '¿que temperatura hace hoy en Teruel?')\n",
    "    \n",
    "def ejemplo3 ():\n",
    "    runPrompt( 'Si En teruel ahora hace 20 grados de temperatura ¿que temperatura hace hoy en teruel?')\n",
    "\n",
    "\n",
    "ejemplo2()\n",
    "ejemplo3()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021088b5-6f4d-4971-86f1-0b609b64f07f",
   "metadata": {},
   "source": [
    "Lo normal es que el ejemplo2 haya respondido diciendo que no puede saberlo.\n",
    "En el caso del ejemplo3, puede ser que varie la respuesta segun el modelo, porque en la pregunta puede  encontrar la respuesta, o no ya que no son muy precisas las instrucciones. No es lo mismo la temperatura de ahora mismo que la temperatura de hoy\n",
    "\n",
    "Es obvio que este tipo de preguntas no suelen hacerse, hacer la pregunta , aportando la respuesta no tiene mucho sentido, pero estamos jugando.\n",
    "\n",
    "Damos un pequeño paso más. Supongamos que al sistema le hayamos informado previamente de las temperaturas, con una orden interna .\n",
    "Ahora puede responder con la información si la tiene.\n",
    "\n",
    "Evidentemente , no es normal hacer esto. Pero es para ilustrar como gradualmente podemos darle información al modelo que le ayude a  dar la respuesta.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c4e73e-fcb9-44d3-b21b-84901a96dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "systemPrompt= 'Has comprobado que en Teruel  hace 15 grados de temperatura y en Cuenca hace 17 grados'\n",
    "\n",
    "def runPrompt2 (prom,syprom):\n",
    "    log('Pregunta:',prom)\n",
    "    response =  client.chat(\n",
    "        model=modelo,\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': syprom},\n",
    "            {'role': 'user', 'content': prom}]        \n",
    "    )\n",
    "    # Extrae la respuesta del modelo\n",
    "    respuesta = response.message.content\n",
    "    log('Respuesta:',respuesta)\n",
    "    return respuesta\n",
    "\n",
    "\n",
    "\n",
    "def ejemplo4 ():\n",
    "    runPrompt2 ( ' ¿que temperatura hace ahora en teruel, cuenca y valencia?',systemPrompt)\n",
    "\n",
    "\n",
    "# ejemplo1()\n",
    "# ejemplo2()\n",
    "#  ejemplo3()\n",
    "ejemplo4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19701c3a-0066-4744-b53d-3aca588c60a9",
   "metadata": {},
   "source": [
    "\n",
    "### Cambiamos de ejemplo. \n",
    "Nos Hemos inventado una operacion matematica que le llamamos mocopo. \n",
    "Como es inventada es normal que el LLM no sepa qué responder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31b4559-0473-4307-abbe-f9ba0393fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ejemploOperacion1 ():\n",
    "    runPrompt ( ' ¿cual es el resultado de hacer la operacion mocopo entre en numero 4 y 8')\n",
    "\n",
    "ejemploOperacion1()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4569df7f-4ce1-4fbe-8949-383cafd99103",
   "metadata": {},
   "source": [
    "Pero  ahora mira este ejemplo: Le estamos haciendo la pregunta y le estamos informando de como se hace la operacion mocopo ,\n",
    "no  le decimos el resultado, le damos la formula para calcularlo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d841f-654b-4362-8893-baba53f3dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ejemploOperacion2 ():\n",
    "    runPrompt2 ( ' ¿cual es el resultado de hacer la operacion mocopo entre el numero 4 y 8',\n",
    "                'la operacion mocopo es  la suma de dos numeros restando 2 al resultado ')\n",
    "\n",
    "ejemploOperacion2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f05da3-2fb5-4509-992b-fb768bcc8fb1",
   "metadata": {},
   "source": [
    "Puedes probar diferentes valores de numeros y seguramente te va a dar respuestas correctas.\n",
    "\n",
    "Esta seria una forma muy primitiva de definir y añadir una funcion nueva al modelo. \n",
    "\n",
    "Podemos hacer prompts mas complejos y detallados, pero nos encontraremos muchas limitaciones, ya que el texto puede tener ambiguedades, o ser demasiado complejos para ser analizados correctamente por el LLM. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0f2b3e-6280-4a77-9d5b-2203dbc839a2",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "Para decirle al modelo ollama  como hacer nuevas operaciones se utiliza algo parecido, pero mas especializado y especifico. Las Tools.\n",
    "Se trata de algo que añadimos a los mensajes , y que el modelo LLM sabe entender y usar.\n",
    "\n",
    "Vamos a ver un ejemplo más complejo que ilustre el ciclo de vida de las tools.\n",
    "* El agente chat hace la consulta al LLM, pasandole la pregunta y diciendole que tiene unas herramientas disponibles por si las necesita.\n",
    "* Si para dar la respuesta , el LLM considera que debe usar alguna de las herramientas, devuelve una respuesta \"especial\" diciendole al agente que necesita usar tal herramienta pasando los datos\n",
    "* El agente le vuelve a pasar la pregunta al LLM, añadiendo los resultados obtenidos por la tool\n",
    "* Si el LLM todavia necesita usar alguna tool más vuelve a responder de forma especial y se repite el ciclo\n",
    "* Cuando el LLM ya tiene todo lo necesario, da la respuesta final. Que es la que llega al usuario.\n",
    "* Todos los mensajes de intercambio de llamadas a  herramientas pueden ser invisibles al usuario, aunque  puedes requerir una confirmacion para proteger un acceso no deseado a alguna tool \n",
    "\n",
    "![image.png](CicloVidaTool.png)\n",
    "\n",
    "\n",
    "Veamos como se hace esto en código python.\n",
    "\n",
    "Primero tenemos definidas unas funciones \"normales\" en python. Lo importante es que esten bien comentados informando de que hacen. Esta descripcion se va a usar para informarle al LLM de que hace la funcion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2fef08-26d8-4bb0-a647-c6430dffac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def temperaturaCiudad (city: str) -> int :\n",
    "  \"\"\"\n",
    "  optiene la temperatura en ciudades de España. Esta funcion se conectaria a un servicio web que devuelve el resultado en tiempo real.\n",
    "  Args:\n",
    "    city (str): la ciudad a consultar\n",
    "  Returns:\n",
    "    int: la temperatura de la ciudad\n",
    "  \"\"\"\n",
    "  # log ('temperaturaCiudad:',city)\n",
    "  if city.lower()=='teruel': return 7\n",
    "  if city.lower()=='cuenca': return 14\n",
    "  return 20\n",
    "\n",
    "\n",
    "def mocopo_two_numbers(a: int, b: int) -> int:\n",
    "  \"\"\"\n",
    "  Mocopo de dos numeros. Es el nombre que hemos inventado para una operacion compleja entre dos numeros\n",
    "  Args:\n",
    "    'a' int : El primer numero\n",
    "    'b' int : El segundo numero\n",
    "   \n",
    "  Returns:\n",
    "    int: el resultado de la operacion\n",
    "  \"\"\"\n",
    "  # log ('mocopo:',a,b)  \n",
    "  return (int(a) + int(b))+(int(a) * int(b))\n",
    "\n",
    "\n",
    "def factorial_par (a: int) -> int:\n",
    "  \"\"\"\n",
    "  FactorialPar de un numero. Un tipo de factorizacion especial definido como a!=a*(a-3)!\n",
    "  Args:\n",
    "    'a' int : El  numero\n",
    "      \n",
    "  Returns:\n",
    "    int: el resultado de la operacion\n",
    "  \"\"\"\n",
    "  a=int(a)\n",
    "  if a<=1: return 1\n",
    "  return a*factorial_par(a-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded7380-a4c7-4e4d-a125-06ef73a77b09",
   "metadata": {},
   "source": [
    "Tambien podemos definir la descripcion aparte usando una nomenclatura concreta. \n",
    "En realidad , la libreria ollama-python que usamos, contruye internamente una descripcion parecida a partir de las funciones que hemos definido. \n",
    "Pero si queremos tener un control más preciso podemos hacerlo nosotros.\n",
    "\n",
    "Un json del siguiente tipo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f076bfd-7d14-479b-9602-9ea4ff96b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ejemplo de definicion de la funcion  mocopo_two_numbers\n",
    "mocopo_two_numbers_tool = {\n",
    "  'type': 'function',\n",
    "  'function': {\n",
    "    'name': 'mocopo_two_numbers',\n",
    "    'description': 'mocopo de dos numeros. Es el nombre que hemos inventado para una operacion compleja entre dos numeros',\n",
    "    'parameters': {\n",
    "      'type': 'object',\n",
    "      'required': ['a', 'b'],\n",
    "      'properties': {\n",
    "        'a': {'type': 'integer', 'description': 'El primer numero'},\n",
    "        'b': {'type': 'integer', 'description': 'el segundo numero'},\n",
    "      },\n",
    "    },\n",
    "  },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ebb76-772b-45f5-84b3-09cbda507617",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c4f9a-17b9-40a3-8999-a8486b5684a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "# diccionario con las funcionciones disponibles\n",
    "\n",
    "funciones_disponibles = {\n",
    "  'temperaturaCiudad': temperaturaCiudad,\n",
    "  'mocopo_two_numbers': mocopo_two_numbers,\n",
    "  'factorial_par':factorial_par\n",
    "}\n",
    "\n",
    "\n",
    "def ComplexChatTool ( pregunta):\n",
    "  # iniciamos la lista de mensajes con la pregunta hecha por el usuario.  \n",
    "  messages = [{'role': 'user', 'content': pregunta}]   \n",
    "  log (\"pregunta \",messages)\n",
    "\n",
    "  # entramos en un bucle del que se sale cuando tenemos la respuesta final  \n",
    "  count = 0\n",
    "  while count < 9:\n",
    "    count+=1\n",
    "    # ejecutamos el chat diciendole que tiene varias herramientas tools disponibles por si las necesita.  \n",
    "    response: ChatResponse = client.chat(\n",
    "          model=modelo,\n",
    "          messages=messages,\n",
    "          tools=[mocopo_two_numbers,temperaturaCiudad,factorial_par]\n",
    "    )\n",
    "    # log ('response: ',count,response)\n",
    "    \n",
    "    # aqui esta el quid principal. En la respuesta del modelo puede hacer una llamada a alguna tool      \n",
    "    # si es asi, ejecutamos las llamadas a esas funciones, añadimos la respuesta a la lista de mensajes\n",
    "    # y volvemos a llamar al chat pasandole la respuesta que necesita. Esto se podria repetir varias veces mientras el modelo necesite más información.\n",
    "    if response.message.tool_calls:  \n",
    "      log (\"ciclo \",count)  \n",
    "      log (response.message.tool_calls)\n",
    "      messages.append(response.message)\n",
    "      for tool in response.message.tool_calls:    \n",
    "\n",
    "        function_to_call = funciones_disponibles.get(tool.function.name)\n",
    "        if function_to_call :\n",
    "          log('Función:', tool.function.name, 'argumentos:', tool.function.arguments)\n",
    "          output = function_to_call(**tool.function.arguments)\n",
    "          log('Function output:', output)\n",
    "          messages.append({'role': 'tool', 'content': str(output), 'tool_name': tool.function.name})  \n",
    "        else:\n",
    "          print('Function', tool.function.name, 'no encontrada')\n",
    "    \n",
    "    else:\n",
    "      # Cuando ya no hay llamadas a tools ya tenemos la respuesta final\n",
    "      #if count<=1: print('No he usado tool calls')\n",
    "      return  response.message.content\n",
    "\n",
    "\n",
    "\n",
    "def ComplexChat ( pregunta):\n",
    "    resulta=ComplexChatTool (pregunta)\n",
    "    print (resulta)\n",
    "    print ('------')\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f05dc9-0cff-4423-a7b1-ac3abf2193ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EjemploComplex1():\n",
    "    ComplexChat (\"donde estan teruel , cuenca y paris\")\n",
    "\n",
    "def EjemploComplex2():\n",
    "    ComplexChat (\"Temperatura de hoy en Teruel y cuanto es el mocopo de 9 y 11\")\n",
    "\n",
    "def EjemploComplex3():\n",
    "    ComplexChat (\"habitantes y temperatura de hoy en Teruel y cuanto es el FactorialPar de 7\")\n",
    "\n",
    "\n",
    "EjemploComplex1()\n",
    "EjemploComplex2()\n",
    "EjemploComplex3()    \n",
    "# EjemploComplex4()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f66fab-414c-4f86-907a-2ab302be5458",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def EjemploComplex5():\n",
    "    ComplexChat (\"Calcula cuanto es el mocopo de 6 y 3. cuando tengas el resultado de la operacion,  calcula el FactorialPar del primer resultado\")\n",
    "\n",
    "def EjemploComplex6():\n",
    "    ComplexChat (\"calcula x =  mocopo de 5 y 3. cuando estes seguro del resultado x, calcula el FactorialPar de x\")\n",
    "\n",
    "EjemploComplex5()    \n",
    "EjemploComplex6()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd7932-8d17-4562-870a-3aef6beeea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def EjemploComplex7():\n",
    "    ComplexChat (\n",
    "        '''t = temperatura en teruel , c= temperatura de cuenca. cuando estes seguro de t y c, calcula x =mocopo de t y c.\n",
    "            Al final, Cuando tengas el resultado x, calcula el FactorialPar de x\n",
    "        ''' )\n",
    "\n",
    "EjemploComplex7()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c3710a-96e8-4b5d-b408-0de34c2ce5c6",
   "metadata": {},
   "source": [
    "![image.png](mocopologo.png)\n",
    "\n",
    "### Felicidades!\n",
    "Si has entendido como funcionan las tools en ollama, ya entiendes como funciona el MCP.\n",
    "* La diferencia es que MCP es un estandar para todos los modelos \n",
    "* En vez de tener que crear agentes especiales para cada modelo, creamos solo uno que sirve para todos.\n",
    "* Ademas, solo tenemos que programar las funciones a publicar, no todo el protocolo de comunicacion, ya que es estandar.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70818778-7e75-48d3-a66d-a7620c9cc29a",
   "metadata": {},
   "source": [
    "# MCP\n",
    "\n",
    "La segunda parte es ver un ejemplo de servicio MCP.\n",
    "\n",
    "Un servicio (o servidor) MCP es un programa que \"publica\" una lista de herramientas y recursos, que pueden ser usados por un Agente MCP, en combinacion de un modelo LLM.\n",
    "\n",
    "En nuestro ejemplo vamos a publicar las funciones  temperaturaCiudad, mocopo_two_numbers y factorial_par\n",
    "Usaremos Claude desktop como agente, y veremos como conectar Claude con nuestras herramientas.\n",
    "\n",
    "\n",
    "\n",
    "Hay 2 formas habituales de hacer la conexion entre el agente  y nuestro servicio MCP\n",
    "\n",
    "### A traves de http. \n",
    "El servicio MCP es muy parecido a un servidor REST tradicional. El servidor atiende a unos comandos concretos estandar MCP, devolviendo informacion en una forma estandar tambien.\n",
    "\n",
    "\n",
    "### Por consola\n",
    "Otra forma es a traves de la linea de comandos. El servidor MCP seria un programa de consola sencillo que comienza esperando que se escriba un texto. Comprueba si el texto es algun comando estandar MCP y si lo es, devuelve el resultado escribiendolo en la consola. Haria lo mismo que hace un servicio http, pero a través de texto en consola en vez de a traves de la red.\n",
    "\n",
    "* La ventaja del primero es que to puedes usar desde otros ordenadores, y el de consola no.\n",
    "* La aventaja de a traves de consola es que puede ser mas rapido de desarrollo o mas fácil de depurar\n",
    "* Los detalles y comandos concretos del protocolo estan especificados en\n",
    "https://modelcontextprotocol.io/docs/getting-started/intro\n",
    "\n",
    "### Problema Claude Evaluación\n",
    "\n",
    "La version gratuida de Claude Desktop, tiene una limitación en cuanto a que MCP puedes conectar.\n",
    "En concreto no pudes conectar servicios MCP a traves de http, solo puedes conectar versiones consola.\n",
    "* Hay formas de saltarse esa limitación usando una especie de proxy que atienda a la linea de comandos , y que conecte con el servidor http que queramos. Pero eso ya son cosas de la tercera parte del curso.\n",
    "\n",
    "\n",
    "## fastmcp\n",
    "Afortunadamente no tenemos que conocer todos los detalles del protocolo MCP.\n",
    "Hay una libreria que nos simplifica todo.\n",
    "* fastmcp : https://github.com/jlowin/fastmcp\n",
    "\n",
    "Adjuntamos un ejemplo en https://./MoCoPoFast1.py\n",
    "\n",
    "\n",
    "Para hacer pruebas de un servidor MCP se puede usar\n",
    "\n",
    "https://modelcontextprotocol.io/legacy/tools/debugging\n",
    "\n",
    "\n",
    "Para usarlo, primero tienes que tener instalado node.js https://nodejs.org/es\n",
    "Vale la pena tener instalado nodejs porque abre la puerta a muchos proyectos abiertos,  entornos web, o leguajes como typeScript\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a468f-bad5-4c5d-914a-162a7488d84b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc043a8-f7a4-403c-ac96-66101dadfd94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
